---
title: "Exercise 2 - Multiple Linear Regression"
output:
  pdf_document: default
  word_document: default
---
## Introduction

In this article we aim to make an accurate linear model prediction of Ozone levels in the atmosphere based on the given dataset. At the beginning we try to gain intuition of the variables by plotting them in pairs and calculate Pearson's correlation coefficient. Then, we construct a simple linear model and study the Adjusted-R2 and R coefficients. We make residuals analysis and Q-Q plots to draw conclusions of the model.
Finally, we follow the best regression selection method by using *regsubsets* command in order to find the best multiple linear model by examine 4 main metrics, RSS, adjusted-R2, Cp and BIC.

## Load Dataset

We first load ozone dataset and store it into df variable. The dataset consists of 14 columns. First column is the *date* of the observation but won't be used in our analysis. 
```{r}
df <- read.table("ozone.txt", header = TRUE, sep=" ")
df <- df[, 2:ncol(df)]
attach(df)
head(df)
```

Second column labeled as *max03* is the depended variable which we would like to predict by constructing a model from the remaining 12 variables, *temperature(T)*, *neon(Ne)*, *Wx*, which had been collected at three different times during the day, and two nominal variables *Wind* and *Rain*.


First we make a simple check for potential missing values in our dataset. 
```{r} 
head(is.na.data.frame(df))
```

## Undestanding the data

Once we have validated the integrity of the dataset, we plot the variables in pairs. We use the built-in command *pairs* which is provided by R. In order to keep our plots simple and readable, we will plot *max03* with the 3 main variables, *Temperature*, *Ne*, *Wx* separately.

```{r correlation-plots, fig.show="hold", out.width="50%"}
pairs(subset(df, select = c(1,2,3,4)))
pairs(subset(df, select = c(1,5,6,7)))
pairs(subset(df, select = c(1,8,9,10)))
pairs(subset(df, select = c(1,11)))
```
The main diagonal is the correlation of each variable with itself. Intuitively, we could say that max03 levels are more correlated with temperature compared to *Wx9* and *Ne*. A statistical method to capture any linear correlation which will be used, is Pearson's coefficient.

### Pearson coefficient

We've seen before that temperature has a strong correlation with *max03*. In order to measure that relationship we will calculate the Pearson correlation coefficient R.
Pearson coefficient measures the strength and direction of a linear relationship between two variables. The value of R is always between +1 and â€“1. Closer to +1 values means there is a very strong positive correlation between variables while closer to -1 a very strong negative correlation. 0 indicates that there is no linear correlation. 

In order to compute coefficient R we use built-in method *cor* and we explicitly ask for pearson method.
```{r}
cor_9 = cor(df$T9, df$maxO3, method = c("pearson"))
cor_12 = cor(df$T12, df$maxO3, method = c("pearson"))
cor_15 = cor(df$T15, df$maxO3, method = c("pearson"))
```
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
  & T9 & T12 & T15\\
\hline
r-coeff(max03) & 0.6993865 & 0.7842623 & 0.77457 \\
\hline
\end{tabular}
\end{center}
We observe that all values (*0.6993865, 0.7842623, 0.77457*) are positive and close to 1 which indicates a strong positive linear relationship. It's worth mentioning that when adding multiple predictors to the model, we care about features with different Pearson coefficients, even in the case with weak correlation because it will increase the performance by learning the data better.

## Linear Regression

### Simple Linear Regression
Previous analysis hinted that *Temperature* variable has the strongest correlation with *max03*. For the purposes of the exercise, we will use *Wx12* as a regressor to construct a simple linear model. A simple linear model is expressed mathematically as shown below.
\newline

\begin{equation} 
 \hat{y} = \beta_{0} + \beta_{1} x + \epsilon
\end{equation}

The objective is to fit a straight line to the data such that the sum of squared errors are minimized.
In R, we simply use the command *lm* (linear model) to fit a linear model to observations and *summary* to get basics statistics of the fit.
```{r}
simple.model <- lm(df$maxO3 ~ df$Wx12)
plot(df$Wx15, df$maxO3, main = "Max03 versus Wx12", xlab = "Wx @ 12:00", ylab = "Ozone levels")
abline(simple.model, col="blue")
```
```{r}
summary(simple.model) 
```
The first metric we are interested in, is R-squared or coefficient of determination. R-squared represents the proportion of variance in dependent variable, in this case *max03* that has been explained by the independent variables, in this case *Wx12* in the model. Is it defined as:

\begin{equation} 
R^{2} = \frac{\text{Variance explained by the model}}{\text{Total variance}}
\end{equation}

In this *simple.model* R-squared is *0.1856* and interpretes as: ~18% of the increase in *max03* levels is due to increase in *Wx12* level. It gives a measure of how well unseen samples are likely to be predicted by the model based on the proportion of explained variance. Larger R-squared values indicate better linear model that fits the observations. Visually, it means that the observed data points are closer to the regression line. 

Limitation of R-squared coefficient is that it does not provide any information whether our model is biased to the data. R-squared can be misleading when you assess the goodness-of-fit for linear regression analysis. A good model could have a low R-squared value which we will deal with it later by performing a residuals plots analysis.

### Residual Plots

In linear regression, the difference between the observed value of the dependent variable and the predicted value, is called *residual*. For each value/point the mathematic expression with one independent variable is shown in 3 which applies for models with multiple regressors as well.

\begin{equation} 
 res = \hat{y} - ( \beta_{0} + \beta x )
\end{equation}

Both the sum and the mean of the residuals are equal to zero. A random scatter of residuals mean that they do not contradict the linear assumption, while distinct curved patterns suggests that a linear model is not a good fit. The following R commands allow us to create the residuals plot and Q-Q plot.

```{r residuals-plots, fig.show="hold", out.width="50%"}
simple.model.residuals = resid(simple.model)
simple.model.fitted = fitted.values(simple.model)
plot(simple.model.fitted, simple.model.residuals, 
          ylab = "Residuals", 
          xlab = "Fitted vaules", 
          main = "Residuals")
abline(0,0)
#create Q-Q plot for residuals
qqnorm(simple.model.residuals)
#add a straight diagonal line to the plot
qqline(simple.model.residuals) 
```
The x-axis on left figure displays the fitted values and the y-axis displays the residuals. From the plot we can see that the spread of the residuals tends to be higher for higher fitted values. Additionally, we can use Q-Q plot to validate the assumption that the residuals follow a normal distribution. Closer to straight line validates this. Though, it is clear that the upper tail tends to stray away from the line.

### Multiple Linear Regression

Previously, we've seen how to assess a linear model with on predictor. In this section we will use multiple regressors to predict *max03* by taking into consideration the 3 variables *T12*, *Ne12* and *Wx12*. Multiple linear regression model is as expressed similarily to (1) but with number of predictors p>1.

\begin{equation} 
 \hat{y} = \beta_{0} + \beta_{1} x_{9} + \beta_{2} x_{12} + \beta_{3} x_{15}
\end{equation}

Matrix notation:

\begin{equation} 
 Y = X \beta + \epsilon
\end{equation}


Similarily to simple model, we use the *lm* command with the addition of two extra variables *T12* and *Ne12*.

```{r}
multi.model <- lm(df$maxO3 ~ df$T12 + df$Ne12 + df$Wx12)
summary(multi.model) 
```

As expected, R-squared value is higher than the *simple.model* as it never decreases when new predictors are added. R-squared is encouraging us to make more complex model for the prediction of *max03*. Though, that would result to *overfitting* and waste of cpu resourses in problems with more variables. Adjusted R-square coefficient is defined as shown below.

\begin{equation}
AdjustedR^{2} = 1 - (1-R^{2})\frac{n - 1}{n-p-1}
\end{equation}
p - number of predictors
\newline
n - sample size.

For every predictor added in the model there is a penalty factor. As the denominator decreases the fraction increases, thus R^{2}-adjusted gets smaller. In case R^{2} is significantly larger with the addition of new regressors then adding new variables to the model was worth it. Next section we will discuss how select the best model for the dataset.

### Model Selection

In this section, we will present a methodology that helps assessing the quality of complex linear models as well as quantitative comparison among different models. We apply the best subset selection approach to the train data. First we create a new dataset containing only numerical features. Before continuing with the implementation it is important define model comparison metrics.

```{r}
df_num <- df[ , purrr::map_lgl(df, is.numeric)]
head(df_num)
```

The best subsets regression, *regsubsets* is a model selection approach that consists of testing all possible combinations of the regressor variables and then selecting the best model according to statistical metrics. Particularily we are interested in, *Adjusted-R2*, *RSS*, *Cp* and *BIC* which are the most commonly used metrics for measuring regression model quality and models comparison.
As mentioned above, *Adjusted-R2* shows the percentage of variation in the outcome that can be explained by predictors variation. *Cp* and *BIC*, address the issue of overfitting, as inevitably more variables added to the model will results to smaller errors. Mathematically are expressed:

\begin{equation}
Cp = \frac{RSS_{p}}{S^{2}} - n + 2(p + 1)
\end{equation}
RSS - Residual sum of squares.
\newline
p - number of predictors.
\newline
n - sample size.

*Residual sum of Squares* are the deviations predicted from actual empirical values of the data. *RSS* is defined such:

\begin{equation}
RSS = \sum_{i=1}^{n} \epsilon_{i} = \sum_{i=1}^{n} y_{i} - (\beta_{0} + \beta x_{i})^{2}
\end{equation}

The objective is to find a model with large *Adjusted-R2* value while keeping *Cp* and *BIC* low.

```{r}
#install.packages("leaps")
library(leaps)
models <- regsubsets(maxO3~., data=df_num, nvmax = 10)
models.summary <- summary(models)
models.summary
```
The below command give us the R-squared value for all possible models with up to 10 predictors. As expected, every time a new variabled is included to the model the value gets higher. The model tends to learn the train set really well and it fails to generalize on new unseen data.
```{r}
models.summary$rsq
```

```{r best-model-plots, fig.show="hold", out.width="50%"}
plot(models.summary$rss , xlab ="Number of Variables", ylab ="RSS ",type ="l")

plot(models.summary$cp ,xlab =" Number of Variables ", ylab =" Cp",type="l")
cp.min <- which.min(models.summary$cp)
points (cp.min, models.summary$cp[cp.min] , col ="purple ", cex =2, pch =20)

plot(models.summary$bic , xlab =" Number of Variables ", ylab =" BIC ",type="l")
bic.min <- which.min (models.summary$bic )
points (bic.min, models.summary$bic[bic.min] , col ="purple ", cex =2, pch =20)

plot(models.summary$adjr2 ,xlab ="Number of Variables",ylab ="Adjusted RSq", type ="l")
adjr2.max <- which.max (models.summary$adjr2)
points (adjr2.max, models.summary$adjr2[adjr2.max] , col ="purple ", cex =2, pch =20)
```

