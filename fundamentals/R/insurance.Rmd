---
title: "Part 2: Exercise 7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
#install.packages("psych",dependencies=TRUE)
#install.packages("corrplot")
#install.packages("randomForest")
#install.packages("randomForestExplainer")
```


## Load Dataset

For the purpose of the exercise we will use the package *psych*. Psych is a package developed for personality, psychometric and psychology research. It provides useful functions for such analysis and it is a core part of International Cognitive Ability Resource (ICAR) project[1].

Dataset consists of 1338 records and 7 features. The column *charges* is the dependent variable, the other 6 will be used to analyze their impact to total costs.

```{r}
require(psych)
require(corrplot)
require(randomForest)
require(randomForestExplainer)
df <- read.csv(file = 'insurance.csv')
nrow(df)
ncol(df)
summary(df)
```

## Describe dataset

Now we will use the *describe* method provided by psych package. It let us for a more in depth overview of the data by presenting the most frequently used descriptive statistics for psychometric and psychology research. Note the symbol * indicates that the variable is categorical.

```{r}
describe(df)
```

We can see about the *mean*, *standard deviation*, *median*, *trimmed*, *mean absolute deviation*, *min*, *max*, *range*, *skew*, *kurtosis* and *standard error*. Before proceeding to model construction that it could explain/predict the dependent variable(*charges*) we need to define skewness and kyrtosis.

### Skewness

Skewness is described as a measure of data symmetry. A perfectly symmetrical data will have a skewness of 0 which might indicate a Normal distribution as the value of skewness for the latter is also 0. 

Skewness is defined as:

\begin{equation}
a_{3} = \sum{\frac{(X_{i} - \bar{X})^{3}}{ns}}
\end{equation}

where:

- *n* is the sample size
- $X_{i}$ is the $i^{th}$ X value
- $\bar{X}$ is the average
- *s* is the sample standard deviation

The exponent *3* is referred to the third standardized central moment for the probability model.

Usually, we interpret the value (rule of thumb) as:

- If the skewness is between -0.5 and 0.5, the data are fairly symmetrical
- If the skewness is between -1 and â€“ 0.5 or between 0.5 and 1, the data are moderately skewed
- If the skewness is less than -1 or greater than 1, the data are highly skewed

### Kyrtosis

Kurtosis is a measure of whether a distribution is narrowly concentrated to the middle; most of the responses are in the center. In other words is a measure of peakedness or flatness of data points.

Kurtosis is defined as:

\begin{equation}
a_{4} = \sum{\frac{(X_{i} - \bar{X})^{4}}{ns}}
\end{equation}

where:

- *n* is the sample size
- $X_{i}$ is the $i^{th}$ X value
- $\bar{X}$ is the average
- *s* is the sample standard deviation

The exponent *4* is referred to the fourth standardized central moment for the probability model.

Analysing the numerical variables of the dataset and the output of *psych.describe* we can see that the variable *bmi* with *skew = 0.28* and *kyrtosis = -0.06* is distributed fairly normally.

### Mean Absolute Devation(MAD)

The mean absolute deviation is the average distance between each data point and the mean. It gives us an idea about the *variability* in a dataset. It is defined as:

\begin{equation}
mad = \frac{\sum{x_{i} - \bar{x}}}{n}
\end{equation}

We observe that *age* has some variability *mad* which could also be conlcuded from standard devation.

### Histograms

We will plot the histograms to get a better visual understanding of numerical variables of the dataset.

```{r hists, figures-side, fig.show="hold", out.width="50%"}
hist(df$age)
hist(df$bmi)
```

```{r hists2, figures-side, fig.show="hold", out.width="50%"}
hist(df$children)
hist(df$charges)
```

Variables *children* and *charges* are skewed, *bmi* as hinted above looks to follow a normal distribution and *age* is a uniform distribution.

## Model

We will test two models, *multivariate linear regression* and *decision tree*. For the first case we need to transform categorical variables into numerical. We use the command *str(structure)* to see the datatypes the set.


### Multivariate Linear Regression
```{r}
str(df)
```

There are 3 features which are categorical. In the snippet above we see the *sex*, *smoker* and *region* have a structure of chr. We need to convert them into *Factors* in order to fit a linear regression model. We call the *as.factor* method.

```{r out.width="50%"}
df$sex <- as.factor(df$sex)
df$smoker <- as.factor(df$smoker)
df$region <- as.factor(df$region)
str(df)
```
### Correlation matrix

Dependent variable seems to have a fairly strong correlation with the *age*.

```{r}
df_num <- df[c(7,1,3,4)]
res <- cor(df_num)
corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

### Fit model

```{r}
linear <- lm(df$charges~df$age + df$bmi + df$children + df$sex + df$smoker + df$region)
summary(linear)
```
Linear regression mke the assumptions of normality of residuals, homoscedasticity[2] (the variability in the response variable is the same at all levels of the explanatory variable) etc. Thus, we need to analyse the residuals of the model.

```{r figures-side, fig.show="hold", out.width="50%"}
linear$predicted <- predict(linear)
linear$residuals <- residuals(linear)
plot(linear, which=1, col=c("blue"))
plot(density(linear$residuals))
qqnorm(linear$residuals, pch = 1, frame = FALSE)
```


Even though the multivariate regression model is able to explain the variance in *charges* achieving $R^2 = 0.7509$ and Adjusted $R^2 = 0.7509$ it fails to full fill the assumption of normality of residuals as is shown above. 

## Decision Trees

In this section we will use a *Random Forest* tree with default hyper-parameters


```{r}
require(randomForest)
# Create a Random Forest model with default parameters
rf <- randomForest(charges ~ ., data = df, importance = TRUE)
rf
```

By default, number of trees is 500 and number of variables tried at each split is 2 in this case. 84% of the variance was explained.

```{r randomForest figures-side, fig.show="hold", out.width="50%"}
min_depth_frame <- min_depth_distribution(rf)
plot_min_depth_distribution(min_depth_frame, mean_sample = "relevant_trees", k = 15)
importance_frame <- measure_importance(rf)
plot_multi_way_importance(importance_frame, size_measure = "no_of_nodes")
plot_multi_way_importance(importance_frame, x_measure = "mse_increase", y_measure = "node_purity_increase", size_measure = "p_value", no_of_labels = 6)
```


We see that feature *smoker* has the higher node_purity_increase and mean-squared-error increase(gini index for classification). We can concluded that the variance of *charges* can be explained mainly from the variable *smoker* as it seems to be the most important feature.

## References

psych package [@https://personality-project.org/r/psych/].

homoscedasticity [@https://en.wikipedia.org/wiki/Homoscedasticity].

